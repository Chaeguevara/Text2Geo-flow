{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Define functions and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-dark-palette')\n",
    "from scipy import stats\n",
    "\n",
    "import datetime as dt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "doc_units = pd.read_excel(\"../../data/processed/units.xlsx\")\n",
    "doc_dict = dict(zip(doc_units[\"from\"],doc_units[\"to\"])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def things_to_unit(a):\n",
    "    \"if 0.5km kind of that appears, convert to unitLength etc\"\n",
    "    for from_ in doc_dict:\n",
    "        idx = np.where(\n",
    "                 np.char.count(a,from_)==1\n",
    "              )\n",
    "        a[idx] = doc_dict[from_] \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer:\n",
    "    ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`','(',')']\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        val = []\n",
    "        for t in word_tokenize(doc):\n",
    "            if t.isdigit():\n",
    "                continue\n",
    "            if (t not in self.ignore_tokens):\n",
    "                val.append(\n",
    "                    self.wnl.lemmatize(t,get_wordnet_pos(t))\n",
    "                )\n",
    "                \n",
    "        return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaPlaceTokenizer:\n",
    "    ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`','(',')']\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        val = []\n",
    "        for t in word_tokenize(doc):\n",
    "            if t.isdigit():\n",
    "                val.append(\"unitN\")\n",
    "            elif (t not in self.ignore_tokens):\n",
    "                val.append(\n",
    "                    self.wnl.lemmatize(t,get_wordnet_pos(t))\n",
    "                )\n",
    "        new_val = np.array(val)\n",
    "        new_val = np.apply_along_axis(things_to_unit, 0, new_val)\n",
    "        return new_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(document):\n",
    "    'changes document to lower case and removes stopwords'\n",
    "\n",
    "    # change sentence to lower case\n",
    "    document = document.lower()\n",
    "\n",
    "    # tokenize into words\n",
    "    words = word_tokenize(document)\n",
    "\n",
    "    # remove stop words & numbrs\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\") or not word.isdigit()]\n",
    "\n",
    "\n",
    "    # join words to make sentence\n",
    "    document = \" \".join(words)\n",
    "    \n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see what are stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../../data/raw/DataCorpus_classfied_중분류_1차 연구.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_type_column(df):\n",
    "    ops = df.iloc[:, 5:]\n",
    "    op_type = ops[ops.columns[1:]].apply(\n",
    "        lambda x: ','.join(x.dropna().astype(str)),\n",
    "        axis=1\n",
    "    )\n",
    "    op_type=op_type.str.lower()\n",
    "    df[\"op_type\"] = op_type\n",
    "    op_type = pd.DataFrame(op_type.unique(), columns=[\"op_type\"])\n",
    "    op_type[\"op_id\"] = op_type.index\n",
    "    op_type.to_excel(\"../../data/raw/op_type.xlsx\")\n",
    "    df = df.merge(op_type, how='left', on=\"op_type\")\n",
    "    df.to_excel(\"../../data/processed/corpora_w_op_type.xlsx\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_type_column(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Type count from raw data\n",
    " Visualize initail labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[9,9])\n",
    "plot = sns.countplot(df[\"op_id\"], palette = 'inferno')\n",
    "for p in plot.patches:\n",
    "    plot.annotate(format(p.get_height(),), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n",
    "\n",
    "plt.title('operation type count', fontdict={'fontsize': 20, 'fontweight': 5, 'color': 'Green'})\n",
    "#plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Drop classes only with a value\n",
    " If a data is the only one in category, dropped <br />\n",
    " 클래스에 해당하는 데이터가 n개 인것은 제거하기로 함. 여기서는 2개 이상으로 설정했음\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop_solo= df.groupby(\"op_id\").filter(lambda x: len(x) >1)\n",
    "df_drop_solo.to_excel(\"../../data/processed/corpora_unique_ops_dropped.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop_solo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[9,9])\n",
    "plot = sns.countplot(df_drop_solo[\"op_id\"], palette = 'inferno')\n",
    "for p in plot.patches:\n",
    "    plot.annotate(format(p.get_height(),), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n",
    "\n",
    "plt.title('operation type count', fontdict={'fontsize': 20, 'fontweight': 5, 'color': 'Green'})\n",
    "#plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Paraphrase only some portion. \n",
    " Stratified sampleing. fraction = 0.8 <br />\n",
    " At this point go to [paraphraser](../../utils/main_balanced.py) and run the script and come back here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_te_set = pd.read_excel(\"../../data/processed/train_test_only_paraphrased.xlsx\")\n",
    "print(tr_te_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[9,9])\n",
    "plot = sns.countplot(tr_te_set[\"op_id\"], palette = 'inferno')\n",
    "for p in plot.patches:\n",
    "    plot.annotate(format(p.get_height(),), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n",
    "\n",
    "plt.title('operation type count', fontdict={'fontsize': 20, 'fontweight': 5, 'color': 'Green'})\n",
    "#plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = tr_te_set[\"Question\"].str.lower()\n",
    "tr_or_test = tr_te_set[\"for train\"]\n",
    "labels = tr_te_set[\"op_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Remove stopword, lemmartization etc...\n",
    " pos tag for stamming <br /> \n",
    " ex) <br /> \n",
    " ***before*** \"The striped bats are hanging on their feet for best\" -> ['The', 'striped', 'bat', 'are', 'hanging', 'on', 'their', 'foot', 'for', 'best'] <br />\n",
    " ***after*** \"The striped bats are hanging on their feet for best\" -> ['The', ***'strip'***, 'bat', ***'be'***, ***'hang'***, 'on', 'their', 'foot', 'for', 'best'] \n",
    " <br />\n",
    " <br />\n",
    " Also ***delete numbers***, cause those are not that important <br />\n",
    " ex) how many buildings are within ***3*** minutes of driving time from fire stations in oleander <br /> -->\n",
    " ['how', 'many', 'building', 'be', 'within', 'minute', 'of', 'drive', 'time', 'from', 'fire', 'station', 'in', 'oleander'] <br />\n",
    " what are the four fire stations within 3 minutes of travel time from a fire in san francisco <br />\n",
    " ['what' 'be' 'the' ***'four'*** 'fire' 'station' 'within' 'unitTime' 'of' 'travel' 'time' 'from' 'a' 'fire' 'in' 'san' 'francisco']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proccessed vs not processed 비교 해볼 것\n",
    "# Not Preprocessed\n",
    "questions_org = [question for question in questions]\n",
    "# preprocessed\n",
    "# questions = [preprocess(question) for question in questions]\n",
    "que_ = questions[5]\n",
    "lemm = LemmaPlaceTokenizer()\n",
    "in_ = que_\n",
    "questions_lem = [\" \".join(lemm(question)) for question in questions]\n",
    "lo_ = 3\n",
    "print(f\"org:{questions_org[lo_]}, \\nlem:{questions_lem[lo_]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Vectorize with TfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer()\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=1, norm='l2', encoding='utf-8',\n",
    " stop_words=None,\n",
    " )\n",
    "features = vectorizer.fit_transform(questions_lem).toarray()\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pd.DataFrame(features, columns = vectorizer.get_feature_names())\n",
    "tfidf_w_question = tfidf.copy()\n",
    "tfidf_w_question[\"Question\"] = tr_te_set[\"Question\"]\n",
    "tfidf_w_question[\"op_id\"] = tr_te_set[\"op_id\"]\n",
    "tfidf_w_question.to_excel(\"../../data/processed/tfidf_vectorized.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "]\n",
    "\n",
    "CV = 2\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "\n",
    "entries = []\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "\n",
    "import seaborn as sns\n",
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "              size=10, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy score\n",
    "cv_df.groupby('model_name').accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bool = tr_te_set[\"for train\"]==True\n",
    "test_bool = ~train_bool\n",
    "X_train_q_tag = tfidf_w_question[train_bool]\n",
    "X_test_q_tag = tfidf_w_question[~train_bool]\n",
    "y_train = X_train_q_tag[\"op_id\"]\n",
    "y_test = X_test_q_tag[\"op_id\"]\n",
    "X_train = X_train_q_tag.drop(columns=[\"Question\",\"op_id\"])\n",
    "X_test = X_test_q_tag.drop(columns=[\"Question\",\"op_id\"])\n",
    "indices_train = y_train.index\n",
    "indices_test = y_test.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick model and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC()\n",
    "model = RandomForestClassifier(n_estimators=300, max_depth=10, random_state=0)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print(sorted(list(set(labels.values))))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=sorted(list(set(labels.values))), \n",
    "            yticklabels=sorted(list(set(labels.values)))\n",
    "            )\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "826053bd674bbf40aaf90a661da74bbf0d2673b0e277479e4b77948c64f16caa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
